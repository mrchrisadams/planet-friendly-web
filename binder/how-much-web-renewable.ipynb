{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How much of the web runs on renewable power?\n",
    "\n",
    "I'm trying to work this out as some research for the Planet Friendly Web Guide, as I can't find much info online myself.\n",
    "\n",
    "Here's my current thinking on how you might do this. There are obvious holes in this, but it's a start, and having _something_ publicly accessible presumably better than what we have now, this is _nowt_.\n",
    "\n",
    "\n",
    "## My current thinking about how to do this\n",
    "\n",
    "- Take a dataset that is representative of the entire web, listing the most popular sites\n",
    "- Find a away to tell if these sites are running on renewable power\n",
    "- Run this check on the sites\n",
    "- Share results\n",
    "\n",
    "I'll explain these in more detail.\n",
    "\n",
    "\n",
    "### Find a dataset that's representative\n",
    "\n",
    "Right now, the de-factor dataset used to represent how the web is used, at least for web performance optimisation and marketing is Alexa's list of the top million sites, as described on [the HTTP Archive site][1].\n",
    "\n",
    "[1]: https://httparchive.org/about.php#faq\n",
    "\n",
    "Iniitally I thought you might need to come up with some on Big Query er, query, like described here on [Ilya Grigorik's website](https://www.igvita.com/2013/06/20/http-archive-bigquery-web-performance-answers/). This saves needing to run our own infra, and should make reproducible.\n",
    "\n",
    "But given we only need the url, and rank for now we might be able to do this without needing to learn big query, or how to make API calls when running a query on the BigQuery.\n",
    "\n",
    "The list of the top million is downloadable as a compressed file, that's about 9mb compressed, and 23mb uncompressed. This is downloadable from an Amazon S3 bucket, at the link below\n",
    "\n",
    "http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\n",
    "\n",
    "### Find a way to tell if it's running on renewable power\n",
    "\n",
    "This in itself is a hellishly complex topic - as there loads of ways to declare that your infrastructure is running on renewable power, from _actually using renewables directly_ to using various financial instruments to buy the same about renewable power you're really using in a non-renewably powered datacentre, to another form of offsets.\n",
    "\n",
    "But for now, to start with, let's try using the [Green Web API](https://www.thegreenwebfoundation.org/green-web-feed/) - the API is simple to understand, and in use.\n",
    "\n",
    "This API checks a domain against who is hosting it, and checks if the hosting company has declared they're running on renewables. It's not clear to me yet what specific checks there, and _how_ they do this, but again, it's our first pass at this.\n",
    "\n",
    "### Run this check\n",
    "\n",
    "It's not technically complex to loop through a csv file, and for each row hit an API. In python the pseudocode might look like this:\n",
    "\n",
    "```python\n",
    "\n",
    "green_or_not_list = []\n",
    "\n",
    "for row in open('path/to/dataset.csv'):\n",
    "    # do some stuff to find the right column, or tell python we're \n",
    "    # looking at a CSV file\n",
    "    site_url = pull_url_column_from(row)\n",
    "    \n",
    "    result = check_against_green_web_api(site_url)\n",
    "    green_or_not_list.push(result)\n",
    "\n",
    "```\n",
    "\n",
    "Thing is this, this involves hitting the API a million times if there are a million rows in the dataset.\n",
    "\n",
    "It's in the very least polite to contact the nice folks at Green Web before we hit their API a million times - one for each url in the Alexa dataset though.\n",
    "\n",
    "The nice thing in the dataset from Alex is that the csv file as two columns, domain and rank, we can do this with the first 10, or maybe 100 results, and get something halfway interesting back, without hosing somebody else's API.\n",
    "\n",
    "### Share Results\n",
    "\n",
    "We should really to make this reproducible.\n",
    "\n",
    "It looks like packaging this up as a [binder][] would be a good step for this. I haven't made one before, but it's worth a try, right?\n",
    "\n",
    "[binder]: https://mybinder.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "req = requests.get('http://s3.amazonaws.com/alexa-static/top-1m.csv.zip', stream=True)\n",
    "\n",
    "with open('top_1m_sites.csv.zip', 'wb') as fd:\n",
    "    for chunk in req.iter_content(chunk_size=128):\n",
    "        fd.write(chunk)\n",
    "\n",
    "req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a zip file, so let's unzip it to see what's inside - the file should be called `top-1m.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile('top_1m_sites.csv.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file with 1 million rows isn't big, but it's not small either, and let's try using pandas to cut it down quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>baidu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>wikipedia.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>reddit.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>qq.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>taobao.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>sohu.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank         domain\n",
       "0     1    youtube.com\n",
       "1     2     google.com\n",
       "2     3   facebook.com\n",
       "3     4      baidu.com\n",
       "4     5  wikipedia.org\n",
       "5     6     reddit.com\n",
       "6     7      yahoo.com\n",
       "7     8         qq.com\n",
       "8     9     taobao.com\n",
       "9    10       sohu.com"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_1m_sites = pd.read_csv('top-1m.csv', header=None, names=['rank', 'domain'])\n",
    "\n",
    "top_10_sites = top_1m_sites.head(10)\n",
    "\n",
    "top_10_sites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have a list, we can add our column to check against the Green Web project, to see if it's running on renewable power or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def check_for_green_power(domain):\n",
    "    \n",
    "    api_url = \"http://api.thegreenwebfoundation.org/greencheck/{}\".format(domain)\n",
    "    res = requests.get(api_url)\n",
    "    json_resp = res.json()\n",
    "    \n",
    "    # lets keep it simple for now\n",
    "    return json_resp['green']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have our dataframe listing all the sites, and a function to check if the domain is running on green power.\n",
    "\n",
    "The final step is to define a third column based on the results of calling `check_for_green_power` on the domain in this row.\n",
    "\n",
    "Sighâ€¦ I've _totally_ forgot how to do this. Let's park it for now, as we need to probably be considerate with thei green web API, and add something at our end to rate limit it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After asking around\n",
    "\n",
    "Ah, after a few false starts here's how to do it. As far as I can tell, you need to do this in two separate steps, otherwise the dataframe starts whining about copying with a message along the lines of:\n",
    "\n",
    "\n",
    "```\n",
    "SettingWithCopyWarning: \n",
    "A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "Try using .loc[row_indexer,col_indexer] = value instead\n",
    "\n",
    "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
    "  \"\"\"Entry point for launching an IPython kernel.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'green'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3889cdc14fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreen_or_not\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_10_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"domain\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_for_green_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgreen_or_not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/data/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e34f2b05d5df>\u001b[0m in \u001b[0;36mcheck_for_green_power\u001b[0;34m(domain)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# lets keep it simple for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson_resp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'green'"
     ]
    }
   ],
   "source": [
    "green_or_not = top_10_sites[\"domain\"].apply(check_for_green_power)\n",
    "\n",
    "green_or_not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our set of results for the list of domains we can then assign it to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_sites.assign(green_or_not=green_or_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_sites.to_csv('top-10-sites-with-greencheck.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, how much of the internet is green?\n",
    "\n",
    "If we take the top 10, we see 4 sites listed as running on renewable power (and three of them are google), and 6 running on fossil fuels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Update, with new data - exciting!\n",
    "\n",
    "So, after getting in touch with Rene at the Greenweb Foundation, I've built a screen scraper to check rather more than 10, so we have a more meaningful dataset.\n",
    "\n",
    "In fact, we're taking the top 1 million domains from Alexa, then checking the top 100k against the this Greenweb API. \n",
    "\n",
    "Because things break on the internet, and running a job like this takes a long time, and some requests might fail, I used the scrapy framework, and wrote a scraper using it, so one failed request doesn't mean I need to start a job from scratch.\n",
    "\n",
    "You can see the [code for the scraper on github](https://github.com/productscience/pfw-top-mil) (pull requests gladly accepted).\n",
    "\n",
    "Anyway, we now have a dataset of which of the [top 100k domains run on green power, and who is hosting them](https://datbase.org/view?query=8be6d29e63b010bb4240dc366e578e1f7f91e89a73062a7802c5b83d9cc793fe).\n",
    "\n",
    "Let's see if we can find anything interesting...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100k_domains = pd.read_csv('top-100k-domains-green-or-not.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>data</th>\n",
       "      <th>error</th>\n",
       "      <th>green</th>\n",
       "      <th>hostedby</th>\n",
       "      <th>hostedbyid</th>\n",
       "      <th>hostedbywebsite</th>\n",
       "      <th>icon</th>\n",
       "      <th>iconurl</th>\n",
       "      <th>partner</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dict</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Google Inc.</td>\n",
       "      <td>595.0</td>\n",
       "      <td>www.google.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dict</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Google Inc.</td>\n",
       "      <td>595.0</td>\n",
       "      <td>www.google.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dict</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>475.0</td>\n",
       "      <td>www.facebook.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dict</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baidu.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dict</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wikipedia.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _type  data error  green     hostedby  hostedbyid   hostedbywebsite icon  \\\n",
       "0  dict  True   NaN   True  Google Inc.       595.0    www.google.com  NaN   \n",
       "1  dict  True   NaN   True  Google Inc.       595.0    www.google.com  NaN   \n",
       "2  dict  True   NaN   True     Facebook       475.0  www.facebook.com  NaN   \n",
       "3  dict  True   NaN  False          NaN         NaN               NaN  NaN   \n",
       "4  dict  True   NaN  False          NaN         NaN               NaN  NaN   \n",
       "\n",
       "  iconurl partner            url  \n",
       "0     NaN     NaN    youtube.com  \n",
       "1     NaN     NaN     google.com  \n",
       "2     NaN     NaN   facebook.com  \n",
       "3     NaN     NaN      baidu.com  \n",
       "4     NaN     NaN  wikipedia.org  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100k_domains.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, there's something interesting here. I'm not too handy with Pandas, but there's something of interest, and I'm going to cheat, but adding a screenshot from some exploration in OpenRefine:\n",
    "\n",
    "![openrefine-screenshot-of-top100k-analysis](./openrefine-screenshot-of-top100k-analysis.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is interesting. According to this, Hetzner is the biggest host of green domains, even bigger than Google. Who knew?\n",
    "\n",
    "I'm not sure where Amazon is on this list, and the criteria we use for deciding whether something is green is something we'd probably need to clarify, but hey - this is a start, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update  with help from friends\n",
    "\n",
    "So, one very helpful [MaikRos](https://github.com/MaikRos) showed me what I needed to do this check using just Pandas by itself. Thanks amigo!\n",
    "\n",
    "I find the image above useful, but sharing the code below makes it easier to reproduce the claims listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hostedby\n",
       "Hetzner Online AG    2358\n",
       "Google Inc.          1942\n",
       "LeaseWeb              465\n",
       "Serverius             315\n",
       "1&1                   280\n",
       "Name: green, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100k_domains.groupby('hostedby').count()['green'].sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
